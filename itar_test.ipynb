{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc1c9752-ee1e-4cd5-8a08-9fa2482a163c",
   "metadata": {},
   "source": [
    "# ITAR Topic Modeling\n",
    "\n",
    "Notes: \n",
    "- This code prints the LLM results in plain text but does not store them in a dataframe, json file, spreadsheet, etc.\n",
    "- This code has not been optimized for performance.\n",
    "- If you run into an Out of Memory (OOO) error, try reducing `batch_size`.\n",
    "- Make sure that `final_batch_size << batch_size`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efc91ae2-cd2f-4a5d-bbc3-b31132e015b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers -q\n",
    "%pip install accelerate -q\n",
    "%pip install openpyxl -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84de4761-04d5-48f9-802c-9ec3352312f0",
   "metadata": {},
   "source": [
    "## Model\n",
    "**IMPORTANT**: This model uses `device_map=auto`. However, in the latest version of `meta-llama/Llama-3.2-3B-Instruct`, setting `device_map=auto` may result in an `RuntimeError: probability tensor contains either inf, nan or element < 0` error. If you encounter this error, set `device_map=cuda`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8eb6dd6-8cfd-4780-b5f7-a2241c8edc88",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15167f7397a340b4823c45573d0a155b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Done]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import pipeline\n",
    "\n",
    "# Hugging Face user token\n",
    "os.environ[\"HF_TOKEN\"]=\"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\"\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"cuda\",\n",
    "    temperature = 0.01,\n",
    "    top_k = 1\n",
    ")\n",
    "transformers.logging.set_verbosity_error()\n",
    "print('[Done]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8098a49d-2e33-4d53-b2c6-b98a5d916782",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3c05531-bade-4298-9d4c-1c6f0da5f11f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num rows: 17620\n",
      "Num rows for HQ Coord Group: 10524\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import openpyxl\n",
    "\n",
    "FILE_XLS = 'x_dnllp_dhs_itar_itar-SME Review-241016.xlsx'\n",
    "df_conditions = pd.read_excel(FILE_XLS, sheet_name='Conditions')\n",
    "print(f'Num rows: {len(df_conditions)}')\n",
    "\n",
    "# Replace all single quotes with double quotes to prevent issues with list of strings below.\n",
    "df_conditions.replace({'\\'': '\"'}, regex=True)\n",
    "\n",
    "df_hq_coord_group = df_conditions.loc[df_conditions['Opened By Group'] == 'ITAR HQ Coordinator Group']\n",
    "print(f'Num rows for HQ Coord Group: {len(df_hq_coord_group)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f289691e-2026-465f-a2eb-e90da794eed2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5eb5ce5-b93f-48d4-88a9-698577acaffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime as dt\n",
    "import pytz\n",
    "\n",
    "\n",
    "def get_elapsed(start):\n",
    "    elapsed = time.time() - start\n",
    "    return dt.strftime(dt.utcfromtimestamp(elapsed), '%Hh:%Mm:%Ss')\n",
    "\n",
    "\n",
    "def get_datetime():\n",
    "    EST = pytz.timezone(\"America/New_York\")\n",
    "    datetime_est = dt.now(EST)\n",
    "    return datetime_est.strftime(\"%m/%d/%Y, %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a285becd-b3ec-489c-9c0e-c9015c218dc6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Topics Inference\n",
    "\n",
    "The following is used to identify the significant topics per batch of Descriptions *AND* the significant topics from the list of topics for each batch of Descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9d9ee3d-ab9b-48f2-b820-be2f1d79f5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(batched_descriptions, num_topics):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"\"\"The following is a list <B> of texts <T>. List the \"\"\" + str(num_topics) + \n",
    "             \"\"\" most significant topics across all texts in <B>. Provide a 2-3 sentence description of each topic including how \n",
    "             many texts out of <B> were relevant to the topic. Do NOT include any additional text. Do not include any XML tags.\"\"\"},\n",
    "        {\"role\": \"user\", \"content\": batched_descriptions},\n",
    "    ]\n",
    "    return messages\n",
    "\n",
    "\n",
    "def get_topics(batched_descriptions, num_topics):\n",
    "    prompt = get_prompt(batched_descriptions, num_topics)\n",
    "    \n",
    "    outputs = pipe(\n",
    "        prompt,\n",
    "        max_new_tokens=512,\n",
    "    )\n",
    "    x = outputs[0][\"generated_text\"][-1]\n",
    "    #print(f'x.get(\"content\"): {x.get(\"content\")}')\n",
    "    return x.get(\"content\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8ec8b6-5243-49c5-90f9-41bed168e633",
   "metadata": {},
   "source": [
    "## Analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d94d90f-8246-44ef-9c61-ac648236a187",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Get Batched Topics\n",
    "- Break down list of Descriptions into a set of `batched_descriptions`, where a `batched_descriptions` is a string of concatenated Descriptions.\n",
    "- For each `batched_descriptions`, generate a description of significant `topics`.\n",
    "- Add each `topics` description to a `batched_description_topics` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a753bc07-f08d-4995-bc06-c9e88b7693fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting... 11/01/2024, 16:13:14\n",
      "- Num records in group: 10524\n",
      "- Batch size: 100\n",
      "- Num topics: 5\n",
      "Processing final batch 106 of 106...\n",
      "len batched_description_topics: 106\n",
      "\n",
      "[End (elapsed: 00h:25m:22s)]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "count = 0\n",
    "num_batches = 0\n",
    "batch_size = 100\n",
    "num_topics = 5\n",
    "batched_descriptions = \"<B>\\n\"\n",
    "batched_description_topics = []\n",
    "\n",
    "print(f'Starting... {(get_datetime())}')\n",
    "print(f'- Num records in group: {len(df_hq_coord_group)}')\n",
    "print(f'- Batch size: {batch_size}')\n",
    "print(f'- Num topics: {num_topics}')\n",
    "\n",
    "total_num_batches = math.ceil(len(df_hq_coord_group) / (batch_size))\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for row, cols in df_hq_coord_group.iterrows():\n",
    "    count += 1\n",
    "    batched_descriptions += \"<T>\" + str(cols['Description']) + \"</T>\\n\"\n",
    "    \n",
    "    if count % batch_size == 0:\n",
    "        # We have a batch\n",
    "        num_batches += 1\n",
    "\n",
    "        print(f'Processing {num_batches} of {total_num_batches} batches...', end='\\r')\n",
    "        batched_descriptions += \"</B>\\n\"\n",
    "        #print(batched_descriptions)\n",
    "        topics = get_topics(batched_descriptions, num_topics)\n",
    "        #print(topics)\n",
    "        batched_description_topics.append(topics)\n",
    "        \n",
    "        # Reset vars\n",
    "        batched_descriptions = \"<B>\\n\"\n",
    "        \n",
    "    elif count == len(df_hq_coord_group):\n",
    "        # We have the last batch\n",
    "        num_batches += 1\n",
    "        print(f'Processing final batch {num_batches} of {total_num_batches}...')\n",
    "        batched_descriptions += \"</B>\\n\"\n",
    "        #print(batched_descriptions)\n",
    "        topics = get_topics(batched_descriptions, num_topics)\n",
    "        #print(topics)\n",
    "        batched_description_topics.append(topics)\n",
    "\n",
    "print(f'len batched_description_topics: {len(batched_description_topics)}')\n",
    "#print(f'batched_description_topics: {batched_description_topics}')\n",
    "print(f'\\n[End (elapsed: {get_elapsed(start)})]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c580213a-0633-4b4d-bab1-0131b5edc8e0",
   "metadata": {},
   "source": [
    "### Batch Significant Topics from Batched Description Topics\n",
    "\n",
    "- Break down `batched_description_topics` list into a set of `batched_topics`, where `batched_topics` is a string of concatenated topics for a batch of Descriptions. \n",
    "- For each `batched_topics`, generate a description of significant `pre_final_topics`.\n",
    "- For `pre_final_topics`, generate the `final_topics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edc2ced8-3760-4890-ac23-674ac9f02b6b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting... 11/01/2024, 16:38:37\n",
      "- Num records in num_batched_description_topics: 106\n",
      "- Final batch size: 25\n",
      "- Final num topics: 5\n",
      "Processing final batch 5 of 5 batched topics...\n",
      "\n",
      "Getting final topics...\n",
      "\n",
      "---------------\n",
      "Final Topics:\n",
      "1. **Procurement Tab Information**: This topic involves entering information into the Procurement Tab, including vendor name, award amount, Contract #/Procurement Instrument Identifier (PIID), and other relevant details. This topic is relevant to 14 texts.\n",
      "\n",
      "2. **Unique Investment Identifier (UII) Information**: This topic involves the entry of UII information into the \"Notes\" tab in the \"Additional Comments\" section of the request's Dashboard, identifying UIIs that are new, changed, or remained the same. This topic is relevant to 14 texts.\n",
      "\n",
      "3. **Contract Award and Procurement Tab**: This topic involves the entry of award information, such as vendor name, award amount, and Contract #/Procurement Instrument Identifier (PIID), into the Procurement tab of the request's Dashboard. This topic is relevant to 8 texts.\n",
      "\n",
      "4. **Procurement Tab Requirements**: This topic involves the requirements for the Procurement Tab, including the entry of vendor name, award amount, and contract number. This topic is relevant to 12 texts.\n",
      "\n",
      "5. **Award Amount and Contract Information**: This topic involves the process of entering award amount and contract information into the request's Submission Form and the request's Dashboard, \"Procurement\" tab. This topic is relevant to 8 texts.\n",
      "\n",
      "[End (elapsed: 00h:01m:39s)]\n"
     ]
    }
   ],
   "source": [
    "final_topics = \"\"\n",
    "batched_topics = \"<B>\\n\"\n",
    "# Note: final_batch_size must be smaller than batch_size.\n",
    "final_batch_size = 25\n",
    "final_num_topics = 5\n",
    "count = 0\n",
    "pre_final_topics = \"\"\n",
    "num_batches = 0\n",
    "\n",
    "num_batched_description_topics = len(batched_description_topics)\n",
    "total_num_batches = math.ceil((num_batched_description_topics) / (final_batch_size))\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "print(f'Starting... {(get_datetime())}')\n",
    "print(f'- Num records in num_batched_description_topics: {num_batched_description_topics}')\n",
    "print(f'- Final batch size: {final_batch_size}')\n",
    "print(f'- Final num topics: {final_num_topics}')\n",
    "\n",
    "for i in range(num_batched_description_topics):\n",
    "    count += 1\n",
    "    batched_topics += \"<T>\" + batched_description_topics[i] + \"</T>\\n\"\n",
    "    \n",
    "    if count % final_batch_size == 0:\n",
    "        # We have a batch\n",
    "        num_batches += 1\n",
    "\n",
    "        print(f'Processing {num_batches} of {total_num_batches} batched topics...', end='\\r')\n",
    "        batched_topics += \"</B>\\n\"\n",
    "        #print(batched_topics)\n",
    "        pre_final_topics += get_topics(batched_topics, final_num_topics)\n",
    "        \n",
    "        # Reset vars\n",
    "        batched_topics = \"<B>\\n\"\n",
    "        \n",
    "    elif count == num_batched_description_topics:\n",
    "        # We have the last batch\n",
    "        num_batches += 1\n",
    "        print(f'Processing final batch {num_batches} of {total_num_batches} batched topics...')\n",
    "        batched_topics += \"</B>\\n\"\n",
    "        #print(batched_topics)\n",
    "        pre_final_topics += get_topics(batched_topics, final_num_topics)\n",
    "\n",
    "        \n",
    "#print(f'pre_final_topics:\\n {pre_final_topics}')\n",
    "\n",
    "final_topics = get_topics(pre_final_topics, final_num_topics)\n",
    "print('\\nGetting final topics...')\n",
    "print(f'\\n---------------\\nFinal Topics:\\n{final_topics}')\n",
    "print(f'\\n[End (elapsed: {get_elapsed(start)})]')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
